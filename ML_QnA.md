## Machine Learning Question & Answer

### 머신러닝에서 과적합(overfitting)이란 무엇이며, 이를 방지하기 위한 방법은 무엇인가?

overfitting은 모델이 학습 데이터에 너무 치중하여 학습 데이터에서는 높은 성능을 보이지만, 새로운 데이터나 테스트 데이터에서는 성능이 저하되는 현상이다. 

이를 방지하기 위한 방법으로 다음과 같은 방법들이 있다.

- early stopping : 학습을 overfitting 되기 전 종료하는 것
- dropout : 학습 수행 시 중간중간에 학습 layer의 일부 activation을 비활성화 시키는 것
- regularization : l1또는 l2 정규화를 통해 모델의 복잡도 감소
- data augmentation : 데이터를 늘려 일반화된 패턴을 학습하도록 하는 것

### Gradient Descent 알고리즘은 무엇이며, 이 알고리즘에서 학습률의 역할은 무엇인가? 또 학습률이 너무 크거나 작을 때 발생할 수 있는 문제는 무엇인지 설명하시오.

Gradient descent 알고리즘은 cost function을 최소화하기 위해 사용되는 최적화 알고리즘이다. 모델의 파라미터에 대한 비용함수의 기울기(gradient)를 계산하고, 그 기울기의 반대 방향으로 파라미터를 업데이트하여 최적점을 찾는다.

learning rate는 각 업데이트 단계에서 파라미터를 얼마나 크게 조정할지 결정하는 하이퍼파라미터이다. 학습률이 너무 크면 학습이 발산하거나 불안정한 학습이 되고, 학습률이 너무 작으면 학습 속도가 매우 느려지고 local minima에 빠질 수 있다.

### 차원의 저주란 무엇이며, 이로 인한 문제를 해결하기 위한 방법은 무엇인지 설명하시오.

차원의 저주는 데이터의 차원이 증가함에 따라 발생하는 문제를 의미한다. 차원이 높아지면 feature space의 부피가 증가하여 데이터가 매우 희소해진다. 이로 인해 학습 알고리즘이 패턴을 효과적으로 학습하기 어려워지고, 모델의 성능이 저하될 수 있다.

이를 해결하기 위한 방법은 다음과 같다.

- 차원 축소 : 주성분 분석, 선형 판별 분석, t-SNE 등을 통해 차원을 축소한다.
- 특징 선택 : 중요한 특징을 선별해서 학습한다.
- 정규화 : l1, l2 정규화 등으로 특징 선별하거나 모델의 복잡도를 낮춘다.

### Backpropagation 알고리즘이 무엇이며, 어떻게 작동하는지 간단히 설명하시오.

신경망의 출력층에서 계산된 오차를 기반으로 출력층부터 입력층까지 역순으로 가중치를 업데이트하는 알고리즘이다. 이 과정에서 각 층의 기여도를 계산하여 가중치를 조정함으로써 모델의 예측 성능을 향상시킨다.

backpropagation이 없으면 deep learning과 같이 multi-layer 구조의 신경망을 학습할 수 없기 때문에 필수적이다.

### 머신러닝에서 Regularization이란 무엇이며, 그 목적과 일반적으로 사용되는 정규화 기법 두 가지를 설명하시오.

Regularization은 학습의 overfitting을 방지하기 위한 기법으로 모델의 복잡도를 줄이기 위해 손실 함수에 페널티를 추가하는 방법이다.

- L1 regularization : 가중치의 절댓값 합을 손실 함수에 추가하여 불필요한 가중치를 0으로 만든다.  이를 통해 특징 선택이 이루어진다.
- L2 regularization : 가중치의 제곱합을 손실 함수에 추가하여 가중치 값이 너무 커지는 것을 방지한다. 이는 모델의 복잡도를 낮추고 일반화 성능을 향상시킨다.

### 딥러닝에서 활성화 함수(activate function)의 역할은 무엇이며, 대표적인 활성화 함수 세 가지를 설명하시오.

활성화 함수는 선형 모델을 비선형 모델로 만들기 위한 함수로 복잡한 패턴과 관계를 학습할 수 있게 한다. 

대표적인 활성화 함수는 다음과 같다.

- Sigmoid : 입력 값을 0 ~ 1 로 변환
- ReLU 함수 : 입력이 0보다 작으면 0, 0보다 크거나 같으면 입력 값을 그대로 출력
- Tanh 함수 : 입력 값을 -1 에서 1 사이의 값으로 변환

### 머신 러닝에서 앙상블 학습이란 무엇이며, 그 장점과 대표적인 기법 두 가지를 설명하시오.

앙상블 학습은 여러개의 모델을 결합하여 단일 모델보다 더 나은 예측 성능을 얻는 방법이다. 일르 통해 모델의 편향과 분산을 줄이고 일반화 성능을 향상시킬 수 있다.

대표적인 기법 두 가지는 다음과 같다.

- 배깅 : 부트스트래핑 기법을 사용하여 원본 데이터 세트에서 복원 추출로 여러 개의 서브 세트를 만든다. 각 서브 세트로 개별 모델을 학습시키고, 최종 예측은 이 모델들의 예측을 평균하거나 투표하여 결정한다.
- 부스팅 : 연속적으로 모델을 학습 시키며, 이전 모델이 예측하지 못한 오류에 집중하여 다음 모델을 개선한다. 각 모델의 예측을 가중 합하여 최종 예측을 만든다.

### 배치 정규화(Batch Normalization)이란 무엇이며, 어떤 문제를 해결하기 위해 사용되는지 설명하시오

신경망의 hidden layer에서의 활성화 값을 정규화하는 기법이다. 배치 정규화는 미니배치 단위로 활성화 값의 평균과 분산을 계산하여 정규화하고, 이를 통해 내부 공변량 변화를 줄인다.
배치 정규화가 해결하는 문제는 다음과 같다.

- 내부 공변량 감소 : 신경망 각 층 입력 분포가 변화하면 학습이 불안정해질 수 있기 때문에 배치 정규화를 통해 각 층의 입력 분포를 일정하게 유지
- 학습 속도 향상: 정규화된 데이터는 더 큰 학습률을 사용할 수 있게 해주어 학습 속도를 높인다.
- 과접합 방지 : 배치 정규화는 일부 규제 효과를 가지고 있어 과적합을 줄이는데 도움을 준다.

### 교차 검증(cross-validation)이란 무엇이며 왜 사용되는지 설명하시오. 일반적으로 사용되는 교차 검증 방법 중 하나를 소개하시오.

데이터를 훈련 데이터와 검증 데이터로 나누어 모델의 성능을 평가하는 방법으로 데이터를 여러 번 나누어 각 분할마다 훈련과 검증을 반복하는 과정을 포함한다.

대표적인 교차 검증 방법으로 K-fold cross-validation이 있다. 이는 데이터를 k개의 fold로 나누어 각 폴드가 한 번씩 검증 데이터로 사용되고 나머지 폴드들이 훈련 데이터로 사용되는 과정을 K번 반복한다. 이를 통해 모델의 성능을 종합적으로 평가할 수 있다. 

### 특징 선택과 특징 추출의 차이점은 무엇이며, 각각의 목적과 방법을 설명하시오.